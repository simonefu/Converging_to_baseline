{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import colors\n",
    "import pickle\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import sklearn.cross_validation\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter \n",
    "import _pickle as cPickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downoad_data(N,name_file):\n",
    "    path = '..\\output\\X' + str(N) + '\\\\'\n",
    "    path_name_file = str(path) + str(name_file) + '.pkl'\n",
    "    file = pd.read_pickle(path_name_file)\n",
    "    return file \n",
    "def upload_data(N,file,name_file):\n",
    "    path = '..\\output\\X' + str(N) + '\\\\'\n",
    "    path_name_file_plk = str(path) + str(name_file) + '.pkl'\n",
    "    path_name_file_csv = str(path) + str(name_file) + '.csv'    \n",
    "    file.to_pickle(path_name_file_plk)\n",
    "    file.to_csv(path_name_file_csv, sep='\\t', encoding='utf-8')\n",
    "    return path_name_file_plk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_parameter(X, test_SIZE):\n",
    "# Use only one feature\n",
    "    X_input = X[['lenght_utterance','distance_to_end','median_duration']]\n",
    "    Y_output = X['duration']\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "# Split the targets into training/testing sets\n",
    "\n",
    "    if test_SIZE == 0:\n",
    "        X_train = X_input\n",
    "        Y_train = Y_output\n",
    "    else:    \n",
    "        X_train, X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(X_input, Y_output, test_size = test_SIZE, random_state = 5)\n",
    "\n",
    "\n",
    "# Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "    regr.fit(X_train, Y_train)\n",
    "    parameters = pd.DataFrame(list(zip(X_input.columns,regr.coef_)), columns= ['features','estimate_coefficients'])\n",
    "    print(parameters)\n",
    "\n",
    "    if test_SIZE == 0:\n",
    "        return regr,parameters, X_train, Y_train\n",
    "    else:\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Make predictions using the testing set\n",
    "        Y_pred = regr.predict(X_test) \n",
    "\n",
    "# The coefficients\n",
    "# The mean squared error\n",
    "        print(\"\\nMean squared error: %.3f\" % mean_squared_error(Y_test, Y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "        print('\\nVariance score: %.2f' % r2_score(Y_test, Y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "        Y_test=pd.DataFrame(Y_test)\n",
    "        result = pd.concat([ Y_test, X_test], axis=1)\n",
    "        index_selected = result.index.values\n",
    "        df = pd.DataFrame({'index': index_selected, 'predicted_duration': Y_pred})\n",
    "        df = df.set_index('index')\n",
    "        result = pd.concat([ df, result], axis=1)\n",
    "        result['distance_to_end'] = pd.to_numeric(result['distance_to_end'], errors='coerce').fillna(0)\n",
    "        result['lenght_utterance'] = pd.to_numeric(result['lenght_utterance'], errors='coerce').fillna(0)\n",
    "    \n",
    "        result.plot(x = 'distance_to_end', y = 'predicted_duration', kind = 'scatter', grid=False,  alpha = 0.15,color='black', figsize= (15,10))\n",
    "        result.plot(x = 'distance_to_end', y = 'duration', kind = 'scatter', grid=False,  alpha = 0.15,color='blue', figsize= (15,10))\n",
    "        plt.xlabel(\"Distance utterance to the end (word)\")\n",
    "        plt.ylabel(\"Duration(s) \")\n",
    "        plt.title(\"Expected & real duration vs distance utterance to the end \")\n",
    "        plt.show()\n",
    "    \n",
    "        return regr,parameters, result, X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_rate_one_conversation(X_processed, id_conv, id_speaker, word_to_exclude_expected, word_to_exclude_real):\n",
    "    \n",
    "    X_filtered_expected = X_processed[(~X_processed['word'].isin(word_to_exclude_expected))]\n",
    "    X_filtered_expected = X_filtered_expected[(X_filtered_expected['ID_conversation'] == id_conv) & (X_filtered_expected['ID_speaker'] == id_speaker)]\n",
    "    \n",
    "    X_filtered_real = X_processed[(~X_processed['word'].isin(word_to_exclude_real))]\n",
    "    X_filtered_real = X_filtered_real[(X_filtered_real['ID_conversation'] == id_conv) & (X_filtered_real['ID_speaker'] == id_speaker)]\n",
    "    \n",
    "    id_utterance = X_filtered_real['ID_utterance'].unique()\n",
    "    for indice_id in id_utterance:\n",
    "        X_filtered_utterance = X_filtered_real[X_filtered_real['ID_utterance'] == indice_id]\n",
    "    \n",
    "        codition_init = True\n",
    "        condition_dataframe_not_empty = True\n",
    "        while codition_init & condition_dataframe_not_empty:\n",
    "            if X_filtered_utterance['word'].iloc[0] in word_to_exclude_expected:\n",
    "                indici = X_filtered_utterance.index\n",
    "                c = indici[0]\n",
    "                X_filtered_real.drop(c, inplace=True)\n",
    "                X_filtered_utterance.drop(c, inplace=True)\n",
    "                condition_dataframe_not_empty = ~X_filtered_utterance.empty\n",
    "            else:\n",
    "                codition_init = False\n",
    "            \n",
    "        codition_init = True\n",
    "        condition_dataframe_not_empty = ~X_filtered_utterance.empty\n",
    "        while codition_init & condition_dataframe_not_empty:\n",
    "            if X_filtered_utterance['word'].iloc[-1] in word_to_exclude_expected:\n",
    "                indici = X_filtered_utterance.index\n",
    "                c = indici[-1]\n",
    "                X_filtered_real.drop(c, inplace=True)\n",
    "                X_filtered_utterance.drop(c, inplace=True)\n",
    "                condition_dataframe_not_empty = ~X_filtered_utterance.empty\n",
    "            else:\n",
    "                codition_init = False       \n",
    "               \n",
    "    \n",
    "    \n",
    "    real_duration = X_filtered_real.groupby(['ID_utterance'])['duration'].sum()\n",
    "   # print(real_duration)\n",
    "    expected_duration = X_filtered_expected.groupby(['ID_utterance'])['expected_duration'].sum()\n",
    "   # print(expected_duration)\n",
    "    pointwise_speech_rate = real_duration/expected_duration\n",
    "    #print(pointwise_speech_rate)\n",
    "    mean_log_pointwise_speech_rate = (np.log(pointwise_speech_rate)).mean() \n",
    "    mean_pointwise_speech_rate = pointwise_speech_rate.mean()\n",
    "    \n",
    "    return [id_conv, id_speaker, mean_log_pointwise_speech_rate, mean_pointwise_speech_rate]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_speech_rate(n_min_words, X_regr, word_to_exclude_expected, word_to_exclude_real, n_directory):\n",
    "    \n",
    "    X = downoad_data(n_directory,'X')\n",
    "    #X_regr = downoad_data(n_directory_regression,'X')\n",
    "    [regr,parameters,X_train, Y_train]= linear_regression_parameter(X_regr, 0)\n",
    "    X['expected_duration'] = regr.predict(X[['lenght_utterance','distance_to_end','median_duration']])\n",
    "    \n",
    "    X_processed1 = X[(~X['word'].isin(word_to_exclude_expected))]\n",
    "\n",
    "    n_significative_utterance = X_processed1.groupby(['ID']).size()\n",
    "    df = pd.DataFrame(n_significative_utterance, columns = {'n_word_significative'})\n",
    "    indici_uterance_significative = df[df['n_word_significative'] > n_min_words  ].index\n",
    "    X_processed_new = X[(X['ID'].isin(indici_uterance_significative))]\n",
    "    X_processed_new.index = np.arange(0, len(X_processed_new))\n",
    "\n",
    "    \n",
    "    list_of_speech_rate = []\n",
    "    conv_tab = pd.read_csv(\"..\\conv_tab.csv\")\n",
    "    indici = conv_tab['id_conv'].as_matrix()\n",
    "    indicistr = []\n",
    "    for i in indici:\n",
    "        indicistr.append(str(i))\n",
    "    \n",
    "    progress = 0\n",
    "\n",
    "    for id_conv in indicistr:\n",
    "\n",
    "        progress = progress + (1/len(indicistr))\n",
    "        update_progress(progress/100)\n",
    "        for id_speaker in ['A','B']:\n",
    "            [id_conv, id_speaker, mean_log_pointwise_speech_rate,mean_pointwise_speech_rate] = speech_rate_one_conversation(X_processed_new,id_conv, id_speaker, word_to_exclude, word_to_exclude_real)\n",
    "            list_of_speech_rate.append([id_conv, id_speaker, mean_log_pointwise_speech_rate,mean_pointwise_speech_rate])\n",
    "        \n",
    "            \n",
    "    speech_rate_speakers = pd.DataFrame(list_of_speech_rate, columns=['id_conv', 'id_caller', 'm_log_pointw_speech_rate','mean_pointwise_speech_rate'])\n",
    "\n",
    "    caller_tab = pd.read_csv(\"..\\caller_tab.csv\")\n",
    "    call_con_tab = pd.read_csv(\"..\\call_con_tab.csv\")\n",
    "    topic_tab = pd.read_csv(\"..\\Topic_tab.csv\")\n",
    "    call_con_tab_selected = call_con_tab[['id_conv','id_caller','id_speaker','id_topic']]\n",
    "    caller_tab_selected = caller_tab[['id_speaker','sex','age','geography','level_study']]\n",
    "    \n",
    "    speaker_tab = call_con_tab_selected.merge(caller_tab_selected, how='left', on = 'id_speaker')\n",
    "    \n",
    "    speech_rate_speakers['id_conv'] = speech_rate_speakers['id_conv'].astype(str).astype(int)\n",
    "\n",
    "    \n",
    "    speech_rate = pd.merge(speech_rate_speakers,speaker_tab, how='left', on = ['id_conv', 'id_caller']  )\n",
    "    speech_rate.drop_duplicates()\n",
    "    speaker_B = speech_rate[['id_conv','id_caller', 'id_speaker']]\n",
    "    speaker_B['id_caller'] = speaker_B['id_caller'].replace(['A'], 1)\n",
    "    speaker_B['id_caller'] = speaker_B['id_caller'].replace(['B'], 0)\n",
    "    speaker_B = speaker_B.sort_values(by = ['id_conv','id_caller'], ascending = [True,True])\n",
    "    speaker_B['id_caller'] = speaker_B['id_caller'].replace([1], 'A')\n",
    "    speaker_B['id_caller'] = speaker_B['id_caller'].replace([0], 'B')\n",
    "    \n",
    "\n",
    "    speaker_B.index = speech_rate.index\n",
    "    speech_rate = pd.merge(speech_rate, speaker_B, left_index=True, right_index=True)\n",
    "\n",
    "    del speech_rate['id_conv_y']\n",
    "    del speech_rate['id_caller_y']\n",
    "\n",
    "    speech_rate.rename(columns={'id_conv_x': 'id_conv', 'id_caller_x': 'id_caller', 'id_speaker_y': 'id_interlocutor', 'id_speaker_x': 'id_speaker'}, inplace=True)\n",
    "    speech_rate['age'] = 1991 - speech_rate['age']\n",
    "    \n",
    "    name_file = 'speech_rate_minword' + str(n_min_words) \n",
    "    upload_data(n_directory,speech_rate,name_file)\n",
    "    \n",
    "    return speech_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "\n",
    "# update_progress() : Displays or updates a console progress bar\n",
    "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
    "## A value under 0 represents a 'halt'.\n",
    "## A value at 1 or bigger represents 100%\n",
    "def update_progress(progress):\n",
    "    barLength = 20 # Modify this to change the length of the progress bar\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"Halt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"Done...\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\rPercent: [{0}] {1}% {2}\".format( \"=\"*block + \" \"*(barLength-block), progress*100, status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # STANDARDIZE\n",
    "def standardize_speech_rate(n_min_words, n_directory):\n",
    "    from sklearn import preprocessing\n",
    "    \n",
    "    name_file = 'speech_rate_minword' + str(n_min_words)\n",
    "    speech_rate = downoad_data(n_directory,name_file)\n",
    "    speech_rate_no_nan = speech_rate[np.isfinite(speech_rate['m_log_pointw_speech_rate'])]\n",
    "    speech_rate_std = speech_rate_no_nan\n",
    "    x = speech_rate_std[['age','m_log_pointw_speech_rate','id_conv']].values #returns a numpy array\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-3,3.1))\n",
    "    #x_scaled = min_max_scaler.fit_transform(x)\n",
    "    x_scaled = preprocessing.scale(x)\n",
    "    numeric_speech_rate_std = pd.DataFrame(x_scaled, index=speech_rate_std.index)\n",
    "    numeric_speech_rate_std.columns = ['age', 'm_log_pointw_speech_rate','id_conv']\n",
    "    speech_rate_std['age_std']  = numeric_speech_rate_std['age']\n",
    "    speech_rate_std['m_log_pointw_speech_rate']  = numeric_speech_rate_std['m_log_pointw_speech_rate']\n",
    "    #speech_rate_std['m_log_pointw_speech_rate'].plot( grid=False,  alpha = 0.3, figsize= (20,15))\n",
    "    \n",
    "    name_file = 'speech_rate_minword' + str(n_min_words) + 'std' \n",
    "    upload_data(n_directory,speech_rate_std, name_file)\n",
    "    return speech_rate_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_speech_rate(N_conv_min_speaker,n_min_words, size_data, n_directory):\n",
    "    \n",
    "    name_file = 'speech_rate_minword' + str(n_min_words) + 'std'\n",
    "    speech_rate_std = downoad_data(n_directory,name_file)\n",
    "    \n",
    "    speech_rate_std = speech_rate_std[speech_rate_std['id_topic'] != ' UNK']\n",
    "    #count_speakerA = speech_rate_std.groupby('id_speaker').count()\n",
    "    #count_speakers_A = count_speakerA[count_speakerA['id_conv'] >= N_conv_min_speaker].index\n",
    "    #count_speakerB = speech_rate_std.groupby('id_interlocutor').count()\n",
    "    #count_speakers_B = count_speakerA[count_speakerA['id_conv'] >= N_conv_min_speaker].index\n",
    "    #speech_rate_filter = speech_rate_std[(speech_rate_std['id_speaker'].isin(count_speakers_A)) & (speech_rate_std['id_interlocutor'].isin(count_speakers_B))  ]\n",
    "    \n",
    "    conv_dati = pd.read_csv(\"..\\conv_tab.csv\")\n",
    "    conv_dati = conv_dati[conv_dati['id_topic'] != ' UNK']\n",
    "    \n",
    "    if size_data == 1:\n",
    "        pass\n",
    "    elif (size_data <= 0.5) or (size_data > 1):\n",
    "        print('\\nError: The size data factor should be in th einterval [0.5, 1]')\n",
    "    else: \n",
    "        N_row_to_drop = int(len(conv_dati) * (1 - size_data) )\n",
    "        id_conv_to_drop = random.sample(set(conv_dati['id_conv']), N_row_to_drop)\n",
    "        conv_dati = conv_dati[-conv_dati['id_conv'].isin(id_conv_to_drop)]\n",
    "                   # df[~df[\"column\"].isin([\"value\"])]\n",
    "    speakers = list(conv_dati['speaker_A'].values)\n",
    "    speakers += list(conv_dati['speaker_B'].values)\n",
    "    c=Counter(speakers) \n",
    "\n",
    "    ID_SPEAKER = list()\n",
    "    COUNT_CONV = list()\n",
    "\n",
    "    for id_speaker in list(set (speakers)):\n",
    "        ID_SPEAKER.append(id_speaker)\n",
    "        COUNT_CONV.append(c[id_speaker])\n",
    "\n",
    "    frame = [('ID_speakers', ID_SPEAKER),\n",
    "         ('number_conv', COUNT_CONV),\n",
    "         ]    \n",
    "    conv_per_speakers_df = pd.DataFrame.from_items(frame)\n",
    "    list_speakers = conv_per_speakers_df[conv_per_speakers_df['number_conv'] >= N_conv_min_speaker].ID_speakers\n",
    "    \n",
    "    speech_rate_filter = speech_rate_std[(speech_rate_std['id_speaker'].isin(list_speakers)) & (speech_rate_std['id_interlocutor'].isin(list_speakers))  ]\n",
    "\n",
    "    \n",
    "    name_file = 'speech_rate_minword' + str(n_min_words) + 'std_' + 'size%_'+ str(size_data*100)+ '_' + str(N_conv_min_speaker) + 'conv_speaker'\n",
    "    upload_data(n_directory, speech_rate_filter, name_file)\n",
    "    return speech_rate_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
