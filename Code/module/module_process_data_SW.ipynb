{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "import pickle\n",
    "import math\n",
    "from datetime import datetime \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data upload:\n",
    "# input: specify the ID of the conversation, the path which contains the data,the speaker ('A' or 'B'), file type('trans' or 'word') \n",
    "# output dataframe of the transcript or the words\n",
    "\n",
    "def download_data1(id_conv, path, speaker, tipo_file):\n",
    "    if tipo_file == 'word':\n",
    "        name_column = 'word'\n",
    "    else: \n",
    "        name_column = 'utterance'\n",
    "    doc = open(path + '\\\\R' + (str(id_conv)[:2]) + '\\\\R' + id_conv + '\\sw' + id_conv + speaker + '-ms98-a-' + tipo_file + '.text')\n",
    "    A = doc.readlines()\n",
    "    df_A = pd.DataFrame(A, columns = ['raw'])\n",
    "    raw = '([a-zA-Z][a-zA-Z](\\d)(\\d)(\\d)(\\d)[a-zA-Z](\\-)[a-zA-Z][a-zA-Z](\\d)(\\d)(\\-)(\\w)(\\-)([0-9]*))'\n",
    "    Label = df_A['raw'].str.extract(raw,expand=True)\n",
    "    Label.rename(columns={'':'label'}, inplace=True)\n",
    "    Label.rename(columns={list(Label)[0]:'label'}, inplace=True)\n",
    "    df_A['raw'] = df_A['raw'].str.replace('\\s+', ' ')\n",
    "    df_A['label'] = Label['label']\n",
    "    df_A['start_time'] = df_A['raw'].str.extract('([0-9]*.\\d\\d\\d\\d\\d\\d\\s[0-9]*.\\d\\d\\d\\d\\d\\d)', expand=True)\n",
    "    df_A['start_time'] = df_A['start_time'].str.replace('(\\s[0-9]*.\\d\\d\\d\\d\\d\\d)', '')\n",
    "    df_A['end_time'] = df_A['raw'].str.extract('([0-9]*.\\d\\d\\d\\d\\d\\d\\s[0-9]*.\\d\\d\\d\\d\\d\\d)', expand=True)\n",
    "    df_A['end_time'] = df_A['end_time'].str.replace('([0-9]*.\\d\\d\\d\\d\\d\\d)\\s', '')\n",
    "    df_A[name_column] = df_A['raw'].str.replace('([a-zA-Z][a-zA-Z](\\d)(\\d)(\\d)(\\d)[a-zA-Z](\\-)[a-zA-Z][a-zA-Z](\\d)(\\d)(\\-)(\\w)(\\-)([0-9]*)(\\s+)([0-9]*.\\d\\d\\d\\d\\d\\d(\\s+)[0-9]*.\\d\\d\\d\\d\\d\\d)\\s+)','')\n",
    "    df_A['start_time'] = pd.to_numeric(df_A['start_time'])\n",
    "    df_A['end_time'] = pd.to_numeric(df_A['end_time'])\n",
    "    return df_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " import time, sys\n",
    "\n",
    "# update_progress() : Displays or updates a console progress bar\n",
    "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
    "## A value under 0 represents a 'halt'.\n",
    "## A value at 1 or bigger represents 100%\n",
    "def update_progress(progress):\n",
    "    barLength = 20 # Modify this to change the length of the progress bar\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"Halt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"Done...\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\rPercent: [{0}] {1}% {2}\".format( \"=\"*block + \" \"*(barLength-block), progress*100, status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function collects all the words in the SWITCHBOARD corpus \n",
    "# INPUT: path where are the data, conv_dati that contains the conversations ID\n",
    "# OUTPUT: data (the entire ensemble of words)\n",
    "def vocabulary(path, conv_dati):\n",
    "    #for id_conv in conv_dati['id_conv']:\n",
    "    from datetime import datetime \n",
    "    startTime= datetime.now()\n",
    "    data = pd.DataFrame(columns=['raw','label','start_time','end_time','word'])\n",
    "    progress = (1/len(conv_dati['id_conv']))\n",
    "    for id_conv in conv_dati['id_conv'].apply(str):    \n",
    "        for speaker in ['A','B']:\n",
    "            data_temp = download_data1(id_conv, path, speaker,'word')\n",
    "            frames = [data, data_temp]\n",
    "            data = pd.concat(frames, ignore_index = True)\n",
    "        progress = progress + (1/len(conv_dati['id_conv']))\n",
    "        update_progress(progress)\n",
    "        \n",
    "    data['word'] = data['word'].str.strip() \n",
    "    ID =  '(\\d\\d\\d\\d)'\n",
    "    ID_utterance =  '(\\-\\d\\d\\d\\d)'\n",
    "    ID_speaker = '(\\d\\d\\d\\d[A-Z])'\n",
    "    ID = '(\\d\\d\\d\\d[A-Z]-[a-z]{2}[0-9]{2}-[a-z]-\\d\\d\\d\\d)'\n",
    "    data['ID_utterance'] = data['label'].str.extract(ID_utterance, expand=True)\n",
    "    data['ID_utterance'] = data['ID_utterance'].str.replace('(\\-)', '')\n",
    "    data['ID_conv'] = data['label'].str.extract(ID, expand=True)\n",
    "    data['ID_speaker'] = data['label'].str.extract(ID_speaker, expand=True)\n",
    "    data['ID_speaker'] = data['ID_speaker'].str.replace('(\\d\\d\\d\\d)', '')\n",
    "    data['ID'] = data['label'].str.extract(ID, expand=True)\n",
    "    data['ID'] = data['ID'].str.replace('(-[a-z]{2}[0-9]{2}-[a-z]-)', '')\n",
    "    data['duration'] = data['end_time'] - data['start_time']\n",
    "    timeElapsed=datetime.now()-startTime \n",
    "    print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))   \n",
    "    \n",
    "    ## to save data\n",
    "    data.to_pickle('..\\output\\data.pkl')\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function computes the counts, the median duration of each word in the vocabulary\n",
    "\n",
    "# input: data (all the words in the vocabulary and its duration)\n",
    "# output: frequency_voc ( median and cpunts for each word)\n",
    "\n",
    "def frequency_median(data): \n",
    "    from datetime import datetime \n",
    "    startTime= datetime.now()\n",
    "    frequency_voc = data.groupby(['word'])['word'].count()\n",
    "    frequency_voc = pd.DataFrame(frequency_voc)\n",
    "    frequency_voc['words'] = frequency_voc.index\n",
    "    frequency_voc.columns = ['counts', 'words']\n",
    "    frequency_voc= frequency_voc.sort_values('counts', ascending = False)\n",
    "    N_word = frequency_voc.count()\n",
    "\n",
    "    frequency_voc['words'] = frequency_voc['words'].str.strip()\n",
    "    frequency_voc.columns = ['counts', 'word']\n",
    "    \n",
    "    progress = (1/len(frequency_voc['word']))\n",
    "    for word in frequency_voc['word']:\n",
    "##for word in ['[silence]\\n','i\\n', 'the\\n' ,'you\\n']:    \n",
    "        temp = data[(data['word'] == word )]\n",
    "        temp['duration'] = temp['end_time'] - temp['start_time']\n",
    "        median_time_word = temp['duration'].median()\n",
    "        frequency_voc.loc[word, 'median_duration'] = median_time_word\n",
    "        \n",
    "        progress = progress + (1/len(frequency_voc['word']))\n",
    "        update_progress(progress)\n",
    "        \n",
    "    timeElapsed=datetime.now()-startTime \n",
    "    print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))        \n",
    "\n",
    "    ### to save frequency_voc\n",
    "    frequency_voc.to_pickle('..\\\\output\\\\frequency_vocabulary.pkl')\n",
    "    return frequency_voc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the percentage of the words covered by: \n",
    "#     the first n word ( specifiyng the int argument 'first_n_word' and setting the int 'N_min_occurence' as None)\n",
    "#     the words which have at least a number min of occurence (specyfing the int argument 'N_min_occurences' and setting int 'first_n_word' as None)\n",
    "#It is possibile to exclude some word specifying the string list 'word_to_exclude'\n",
    "\n",
    "def frequency_word_analysis(frequency_voc, N_min_occurences, word_to_exclude, first_n_word):\n",
    "    \n",
    "    N_Tot = frequency_voc['counts'].sum()\n",
    "    \n",
    "    if (first_n_word is None) & (N_min_occurences is not None):    \n",
    "        frequency_filter = frequency_voc[(frequency_voc['counts'] >= N_min_occurences) & (~frequency_voc['word'].isin(word_to_exclude))]\n",
    "    elif (first_n_word is not None) & (N_min_occurences is None):\n",
    "        frequency_filter = frequency_voc[(~frequency_voc['word'].isin(word_to_exclude))]\n",
    "        frequency_filter = frequency_filter.head(first_n_word)\n",
    "    else:\n",
    "        print('error')\n",
    "        return\n",
    "    \n",
    "    L = frequency_filter.count()\n",
    "    L_Tot = frequency_filter['counts'].sum()\n",
    "    perc_vocabulary = (L_Tot/N_Tot)*100\n",
    "    \n",
    "    print('\\nFraction of the vocabulary covered by %d word = 0.%d, excluding %s ' % (L[1], perc_vocabulary, word_to_exclude))\n",
    "    plt.figure(1)\n",
    "    frequency_filter['counts'].plot(kind = 'bar', grid=False,  alpha = 0.3, figsize= (20,15))\n",
    "    plt.xlabel('word') \n",
    "    plt.ylabel('count')\n",
    "    plt.title('Word Frequency Vocabulary SWITCHBOARD')\n",
    "    plt.figure(2)\n",
    "    frequency_filter['median_duration'].plot(kind = 'bar', grid=False,  alpha = 0.3, figsize= (20,15))\n",
    "    plt.xlabel('word') \n",
    "    plt.ylabel('time(s)')\n",
    "    plt.title('Median words SWITCHBOARD')\n",
    "    plt.show()\n",
    "\n",
    "    return frequency_filter, perc_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function compute a dataframe with the following information ['word','ID','lenght_utterance','distance_to_end','duration']\n",
    "# input: frequency_voc, data, word_to_exclude(array of words to be excluded), n (the minimum lenght to take into account an utterance)\n",
    "def processing_data(frequency_voc, data, word_to_exclude, n):\n",
    "    from datetime import datetime \n",
    "\n",
    "    startTime= datetime.now()\n",
    "    cols = ['word']\n",
    "    frequency_voc_median = frequency_voc[['word', 'median_duration']]\n",
    "    X = pd.DataFrame(columns=['word','ID','lenght_utterance','distance_to_end','duration','start_time','end_time'])\n",
    "    data = data[(~data['word'].isin(word_to_exclude))]\n",
    "    data = data[['duration', 'ID','word','start_time','end_time']]\n",
    "    for ID in data.ID.unique():\n",
    "        data_filter = data[(data['ID'] == ID )]\n",
    "        N = data_filter['word'].count()\n",
    "        if N >= n:\n",
    "            data_filter['lenght_utterance'] = data_filter['word'].count()\n",
    "            distance = list(range(0,N))\n",
    "            distance = distance[::-1]\n",
    "            data_filter['distance_to_end'] = distance\n",
    "            data_filter = data_filter.join(frequency_voc_median.set_index(cols), on=cols, sort = False)\n",
    "        else:\n",
    "            continue\n",
    "        frames = [X, data_filter]\n",
    "        X = pd.concat(frames, ignore_index = True)\n",
    "\n",
    "        \n",
    "    timeElapsed=datetime.now()-startTime \n",
    "    print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "   \n",
    "    return X\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function divides the dataframe DATA in intervals of size about n_each_interval(int)\n",
    "\n",
    "def divide_interval_data(data, n_each_interval):\n",
    "    i = 0 \n",
    "    indices = [0]\n",
    "    while(i < data.index[-1] - n_each_interval):\n",
    "        i = i + n_each_interval\n",
    "        while(data.iloc[i]['ID'] == data.iloc[i+1]['ID']):\n",
    "            i = i + 1\n",
    "        indices.append(i)\n",
    "    indices.append(data.index[-1])\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dataframe(indices, n_directory):\n",
    "    dataframe = []\n",
    "    for i in range(len(indices) - 1):\n",
    "        name_file = '..\\\\output\\\\X' + str(n_directory) + '\\\\X_' + str(i) + '.pkl'\n",
    "        temp_X = pd.read_pickle(name_file)\n",
    "        dataframe.append(temp_X)\n",
    "        \n",
    "    while(len(dataframe) > 1):\n",
    "        indices_delete = []\n",
    "        if len(dataframe) % 2 == 1:\n",
    "            n = len(dataframe) - 1 \n",
    "        else:\n",
    "            n = len(dataframe)\n",
    "        for i in range(0,n,2):\n",
    "            frames = [dataframe[i], dataframe[i+1]]\n",
    "            dataframe[i] = pd.concat(frames, ignore_index = True)\n",
    "        dataframe = dataframe[0:len(dataframe):2]\n",
    "        name_file_to_save = '..\\\\output\\\\X' + str(n_directory) + '\\\\X' + '.pkl'\n",
    "        dataframe[0]['ID_conversation'] = dataframe[0]['ID'].str.extract('(\\d\\d\\d\\d)', expand=True) \n",
    "        dataframe[0]['ID_speaker'] = dataframe[0]['ID'].str.extract('([A-Z])', expand=True)\n",
    "        dataframe[0]['ID_utterance'] = dataframe[0]['ID'].str.extract('(\\d\\d\\d\\d$)', expand=True)\n",
    "\n",
    "        dataframe[0].to_pickle(name_file_to_save)\n",
    "    return dataframe     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
